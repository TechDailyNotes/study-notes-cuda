{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpeV0y8KKSxQjDXSsJjeFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechDailyNotes/study-notes-cuda/blob/main/cuda_parallel_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXvBFi2RiMuJ",
        "outputId": "03968527-1069-4197-855a-adbf547d3a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpulgv1rcd\".\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1: CUDA Parallel Reduction Part 1"
      ],
      "metadata": {
        "id": "rhHi1_AllhUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <assert.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "const int GRID_DIM_X = 1 << 8;\n",
        "const int BLOCK_DIM_X = 1 << 8;\n",
        "\n",
        "__global__ void sumReduce(int *vector, int *vectorSum) {\n",
        "    // Step 0: Get the current thread's index.\n",
        "    int ti = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Step 1: Move elements from memory to cache.\n",
        "    __shared__ int partialSum[BLOCK_DIM_X];\n",
        "    partialSum[threadIdx.x] = vector[ti];\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Divide and conquer the sum in one block.\n",
        "    for (int si = 1; si < BLOCK_DIM_X; si *= 2) {\n",
        "        if (threadIdx.x % (si * 2) == 0) {\n",
        "            partialSum[threadIdx.x] += partialSum[threadIdx.x + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Step 3: Move the sum from cache to memory.\n",
        "    if (threadIdx.x == 0) {\n",
        "        vectorSum[blockIdx.x] = partialSum[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "void vectorInit(int *h_vector, int numElements) {\n",
        "    for (int i = 0; i < numElements; i++) {\n",
        "        h_vector[i] = 1;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 0: Set the number and bytes of the vector.\n",
        "    int numElements = GRID_DIM_X * BLOCK_DIM_X;\n",
        "    size_t numBytes = sizeof(int) * numElements;\n",
        "\n",
        "    // Step 1: Initialize the host and device memories.\n",
        "    int *h_vector = (int*) malloc(numBytes);\n",
        "    int *h_vectorSum = (int*) malloc(numBytes);\n",
        "    vectorInit(h_vector, numElements);\n",
        "\n",
        "    int *d_vector, *d_vectorSum;\n",
        "    cudaMalloc(&d_vector, numBytes);\n",
        "    cudaMalloc(&d_vectorSum, numBytes);\n",
        "\n",
        "    // Step 2: Launch the kernel function to sum up the vector.\n",
        "    cudaMemcpy(d_vector, h_vector, numBytes, cudaMemcpyHostToDevice);\n",
        "    sumReduce<<<GRID_DIM_X, BLOCK_DIM_X>>>(d_vector, d_vectorSum);\n",
        "    sumReduce<<<1, BLOCK_DIM_X>>>(d_vectorSum, d_vectorSum);\n",
        "    cudaMemcpy(h_vectorSum, d_vectorSum, numBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"h_vectorSum[0] == %d\\n\", h_vectorSum[0]);\n",
        "    assert(h_vectorSum[0] == 65536);\n",
        "\n",
        "    // Step 3: Clear the allocated memories.\n",
        "    free(h_vector);\n",
        "    free(h_vectorSum);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_vectorSum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV9IibY_ia77",
        "outputId": "0d135264-1249-465c-ce58-0615a9b1b495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h_vectorSum[0] == 65536\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice"
      ],
      "metadata": {
        "id": "zCS_QCo6j1we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "const int GRID_DIM_X = 1 << 8;\n",
        "const int BLOCK_DIM_X = 1 << 8;\n",
        "\n",
        "__global__ void sumReduce(int *d_vector, int *d_vectorSum) {\n",
        "    // Step 0: Get the thread id and element id.\n",
        "    int vi = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    int ti = threadIdx.x;\n",
        "\n",
        "    // Step 1: Move elements from the vector to the cache.\n",
        "    __shared__ int partialSum[BLOCK_DIM_X];\n",
        "    partialSum[ti] = d_vector[vi];\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Sum all elements in the same block.\n",
        "    for (int si = 1; si < BLOCK_DIM_X; si *= 2) {\n",
        "        if (ti % (2 * si) == 0) {\n",
        "            partialSum[ti] += partialSum[ti + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Step 3: Move the sum value to the vector.\n",
        "    if (threadIdx.x == 0) {\n",
        "        d_vectorSum[blockIdx.x] = partialSum[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "void vectorInit(int *h_vector, int numElements) {\n",
        "    fill_n(h_vector, numElements, 1);\n",
        "    // memset(h_vector, 1, numElements);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 0: Set the hyperparameters of vectors.\n",
        "    int numElements = GRID_DIM_X * BLOCK_DIM_X;\n",
        "    size_t numBytes = sizeof(int) * numElements;\n",
        "\n",
        "    // Step 1: Initialize memories for vectors in both the host and device.\n",
        "    int *h_vector = (int*) malloc(numBytes);\n",
        "    int *h_vectorSum = (int*) malloc(numBytes);\n",
        "    vectorInit(h_vector, numElements);\n",
        "\n",
        "    int *d_vector, *d_vectorSum;\n",
        "    cudaMalloc(&d_vector, numBytes);\n",
        "    cudaMalloc(&d_vectorSum, numBytes);\n",
        "\n",
        "    // Step 2: Launch the kernel function to sum up all elements.\n",
        "    cudaMemcpy(d_vector, h_vector, numBytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    time_t start = time(NULL);\n",
        "    sumReduce<<<GRID_DIM_X, BLOCK_DIM_X>>>(d_vector, d_vectorSum);\n",
        "    sumReduce<<<1, BLOCK_DIM_X>>>(d_vectorSum, d_vectorSum);\n",
        "    time_t end = time(NULL);\n",
        "    printf(\"Time taken is %f seconds.\\n\", difftime(end, start));\n",
        "\n",
        "    cudaMemcpy(h_vectorSum, d_vectorSum, numBytes, cudaMemcpyDeviceToHost);\n",
        "    printf(\"h_vectorSum[0] == %d\\n\", h_vectorSum[0]);\n",
        "\n",
        "    // Step 3: Clear allocated memories.\n",
        "    free(h_vector);\n",
        "    free(h_vectorSum);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_vectorSum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHhC3uTlj3Iv",
        "outputId": "92cf6620-f2c4-412a-def0-33c4222b52d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken is 0.000000 seconds.\n",
            "h_vectorSum[0] == 65536\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2: CUDA Parallel Reduction Part 2\n",
        "\n",
        "## Optimizations\n",
        "\n",
        "1. Get rid of the wrap divergence.\n",
        "2. Get rid of the modulo operation."
      ],
      "metadata": {
        "id": "EMKLo5RdvLa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "\n",
        "const int GRID_DIM_X = 1 << 8;\n",
        "const int BLOCK_DIM_X = 1 << 8;\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *vectorSum) {\n",
        "    // Step 0: Get the vector index and the thread index.\n",
        "    int ti = threadIdx.x;\n",
        "    int vi = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Step 1: Move elements from the vector to the cache.\n",
        "    __shared__ int partialSum[BLOCK_DIM_X];\n",
        "    partialSum[ti] = vector[vi];\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Accumulate all elements.\n",
        "    for (int si = 1; si < blockDim.x; si *= 2) {\n",
        "        int index = 2 * si * ti;\n",
        "        if (index < blockDim.x) {\n",
        "            partialSum[index] += partialSum[index + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Step 3: Move the sum to the vector.\n",
        "    if (ti == 0) {\n",
        "        vectorSum[blockIdx.x] = partialSum[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 0: Setup the parameters.\n",
        "    int numElements = GRID_DIM_X * BLOCK_DIM_X;\n",
        "    size_t numBytes = sizeof(int) * numElements;\n",
        "\n",
        "    // Step 1: Initialize both the cpu and gpu memories.\n",
        "    int *h_vector = (int*) malloc(numBytes);\n",
        "    int *h_vectorSum = (int*) malloc(numBytes);\n",
        "    std::fill_n(h_vector, numElements, 1);\n",
        "\n",
        "    int *d_vector, *d_vectorSum;\n",
        "    cudaMalloc(&d_vector, numBytes);\n",
        "    cudaMalloc(&d_vectorSum, numBytes);\n",
        "\n",
        "    // Step 2: Launch the reduce sum kernel function.\n",
        "    cudaMemcpy(d_vector, h_vector, numBytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    time_t start, end;\n",
        "    time(&start);\n",
        "    reduceSum<<<GRID_DIM_X, BLOCK_DIM_X>>>(d_vector, d_vectorSum);\n",
        "    reduceSum<<<1, BLOCK_DIM_X>>>(d_vectorSum, d_vectorSum);\n",
        "    time(&end);\n",
        "    printf(\"Time taken is %f seconds.\\n\", difftime(end, start));\n",
        "\n",
        "    cudaMemcpy(h_vectorSum, d_vectorSum, numBytes, cudaMemcpyDeviceToHost);\n",
        "    printf(\"Accumulated result is %d.\\n\", h_vectorSum[0]);\n",
        "\n",
        "    // Step 3: Clear allocated memories.\n",
        "    free(h_vector);\n",
        "    free(h_vectorSum);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_vectorSum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t5aIdgMvPqq",
        "outputId": "c54e585b-7d8e-46ab-e582-5d6c94cdfcc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken is 0.000000 seconds.\n",
            "Accumulated result is 65536.\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3: CUDA Parallel Reduction Part 3\n",
        "\n",
        "## Optimizations\n",
        "\n",
        "1. Get rid of the bank conflict."
      ],
      "metadata": {
        "id": "pglzPtt59jsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <algorithm>\n",
        "#include <assert.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "const int GRID_DIM_X = 1 << 8;\n",
        "const int BLOCK_DIM_X = 1 << 8;\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *vectorSum) {\n",
        "    // Step 0: Get the vector element index and the thread index.\n",
        "    int ti = threadIdx.x;\n",
        "    int vi = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Step 1: Move vector elements to cache.\n",
        "    __shared__ int shared[BLOCK_DIM_X];\n",
        "    shared[ti] = vector[vi];\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Accumulate elements in the same thread block.\n",
        "    for (int si = BLOCK_DIM_X / 2; si > 0; si >>= 1) {\n",
        "        if (ti < si) {\n",
        "            shared[ti] += shared[ti + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Step 3: Move result from cache to vector.\n",
        "    if (ti == 0) {\n",
        "        vectorSum[blockIdx.x] = shared[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 0: Set up hyperparameters of the vector.\n",
        "    int numElements = GRID_DIM_X * BLOCK_DIM_X;\n",
        "    size_t numBytes = sizeof(int) * numElements;\n",
        "\n",
        "    // Step 1: Init vector memories on both the cpu and the gpu.\n",
        "    int *h_vector = (int*) malloc(numBytes);\n",
        "    int *h_vectorSum = (int*) malloc(numBytes);\n",
        "    std::fill_n(h_vector, numElements, 1);\n",
        "\n",
        "    int *d_vector, *d_vectorSum;\n",
        "    cudaMalloc(&d_vector, numBytes);\n",
        "    cudaMalloc(&d_vectorSum, numBytes);\n",
        "\n",
        "    // Step 2: Launch the kernel function to sum up the vector.\n",
        "    cudaMemcpy(d_vector, h_vector, numBytes, cudaMemcpyHostToDevice);\n",
        "    reduceSum<<<GRID_DIM_X, BLOCK_DIM_X>>>(d_vector, d_vectorSum);\n",
        "    reduceSum<<<1, BLOCK_DIM_X>>>(d_vectorSum, d_vectorSum);\n",
        "    cudaMemcpy(h_vectorSum, d_vectorSum, numBytes, cudaMemcpyDeviceToHost);\n",
        "    printf(\"Accumulated result is %d.\\n\", h_vectorSum[0]);\n",
        "    assert(h_vectorSum[0] == 65536);\n",
        "\n",
        "    // Step 3: Clear vector memories.\n",
        "    free(h_vector);\n",
        "    free(h_vectorSum);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_vectorSum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "P1LUaZnF0Ai-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9457aa-d5dd-4a65-e850-74786d0aee55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated result is 65536.\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4: CUDA Parallel Reduction Part 4\n",
        "\n",
        "## Optimizations\n",
        "\n",
        "1. Get rid of idle threads."
      ],
      "metadata": {
        "id": "wpXXWhd6jN1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <algorithm>\n",
        "#include <assert.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "const int FACTOR = 2;\n",
        "const int GRID_DIM_X = (1 << 8) / FACTOR;\n",
        "const int BLOCK_DIM_X = 1 << 8;\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *vectorSum) {\n",
        "    // Step 0: Get the vector element index and the thread index.\n",
        "    int ti = threadIdx.x;\n",
        "    int vi = blockIdx.x * (blockDim.x * 2) + threadIdx.x;\n",
        "\n",
        "    // Step 1: Move vector elements to cache.\n",
        "    __shared__ int shared[BLOCK_DIM_X];\n",
        "    shared[ti] = vector[vi] + vector[vi + blockDim.x];\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Accumulate elements in the same thread block.\n",
        "    for (int si = BLOCK_DIM_X / 2; si > 0; si >>= 1) {\n",
        "        if (ti < si) {\n",
        "            shared[ti] += shared[ti + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Step 3: Move result from cache to vector.\n",
        "    if (ti == 0) {\n",
        "        vectorSum[blockIdx.x] = shared[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 0: Set up hyperparameters of the vector.\n",
        "    int numElements = GRID_DIM_X * BLOCK_DIM_X * FACTOR;\n",
        "    size_t numBytes = sizeof(int) * numElements;\n",
        "\n",
        "    // Step 1: Init vector memories on both the cpu and the gpu.\n",
        "    int *h_vector = (int*) malloc(numBytes);\n",
        "    int *h_vectorSum = (int*) malloc(numBytes);\n",
        "    std::fill_n(h_vector, numElements, 1);\n",
        "\n",
        "    int *d_vector, *d_vectorSum;\n",
        "    cudaMalloc(&d_vector, numBytes);\n",
        "    cudaMalloc(&d_vectorSum, numBytes);\n",
        "\n",
        "    // Step 2: Launch the kernel function to sum up the vector.\n",
        "    cudaMemcpy(d_vector, h_vector, numBytes, cudaMemcpyHostToDevice);\n",
        "    reduceSum<<<GRID_DIM_X, BLOCK_DIM_X>>>(d_vector, d_vectorSum);\n",
        "    reduceSum<<<1, GRID_DIM_X / 2>>>(d_vectorSum, d_vectorSum);\n",
        "    cudaMemcpy(h_vectorSum, d_vectorSum, numBytes, cudaMemcpyDeviceToHost);\n",
        "    printf(\"Accumulated result is %d.\\n\", h_vectorSum[0]);\n",
        "    // assert(h_vectorSum[0] == 65536);\n",
        "\n",
        "    // Step 3: Clear vector memories.\n",
        "    free(h_vector);\n",
        "    free(h_vectorSum);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_vectorSum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFmllZKnjNcN",
        "outputId": "7f199c69-28d3-4882-a669-465b9cbc27fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated result is 65536.\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice"
      ],
      "metadata": {
        "id": "wT9ecjsTmzjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "const int FACTOR = 2;\n",
        "const int GRID_DIM_X = (1 << 8) / FACTOR;\n",
        "const int BLOCK_DIM_X = 1 << 8;\n",
        "const int NUM_ELEMENTS = GRID_DIM_X * BLOCK_DIM_X * FACTOR;\n",
        "const size_t NUM_BYTES = sizeof(int) * NUM_ELEMENTS;\n",
        "const size_t NUM_BYTES_PARTIAL = sizeof(int) * GRID_DIM_X;\n",
        "const size_t NUM_BYTES_SUM = sizeof(int);\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *vectorSum) {\n",
        "    // Step 0: Get the vector index and the thread index.\n",
        "    int ti = threadIdx.x;\n",
        "    int vi = blockIdx.x * (blockDim.x * 2) + threadIdx.x;\n",
        "\n",
        "    // Step 1: Move elements from the vector to the cache.\n",
        "    __shared__ int shared[BLOCK_DIM_X];\n",
        "    shared[ti] = vector[vi] + vector[vi + blockDim.x];\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Accumulate vector elements.\n",
        "    for (int si = BLOCK_DIM_X / 2; si > 0; si >>= 1) {\n",
        "        if (ti < si) {\n",
        "            shared[ti] += shared[ti + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Step 3: Move results from the cache to the vector.\n",
        "    if (ti == 0) {\n",
        "        vectorSum[blockIdx.x] = shared[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 1: Init vector memories on both the cpu and the gpu.\n",
        "    int *h_vector = (int*) malloc(NUM_BYTES);\n",
        "    int *h_vectorSum = (int*) malloc(NUM_BYTES_SUM);\n",
        "    std::fill_n(h_vector, NUM_ELEMENTS, 1);\n",
        "\n",
        "    int *d_vector, *d_vectorPartialSum, *d_vectorSum;\n",
        "    cudaMalloc(&d_vector, NUM_BYTES);\n",
        "    cudaMalloc(&d_vectorPartialSum, NUM_BYTES_PARTIAL);\n",
        "    cudaMalloc(&d_vectorSum, NUM_BYTES_SUM);\n",
        "\n",
        "    // Step 2: Launch the kernel function to accumulate values.\n",
        "    cudaMemcpy(d_vector, h_vector, NUM_BYTES, cudaMemcpyHostToDevice);\n",
        "    reduceSum<<<GRID_DIM_X, BLOCK_DIM_X>>>(d_vector, d_vectorPartialSum);\n",
        "    reduceSum<<<1, GRID_DIM_X / 2>>>(d_vectorPartialSum, d_vectorSum);\n",
        "    cudaMemcpy(h_vectorSum, d_vectorSum, NUM_BYTES_SUM, cudaMemcpyDeviceToHost);\n",
        "    printf(\"Accumulated result is %d.\\n\", *h_vectorSum);\n",
        "\n",
        "    // Step 3: Clear vector memories.\n",
        "    free(h_vector);\n",
        "    free(h_vectorSum);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_vectorPartialSum);\n",
        "    cudaFree(d_vectorSum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYaAbIj0m1cy",
        "outputId": "0608815f-c55c-41c3-edc8-5d2e9c447874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated result is 65536.\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5: CUDA Parallel Reduction Part 5\n",
        "\n",
        "## Optimizations\n",
        "\n",
        "1. Release idle thread warps early."
      ],
      "metadata": {
        "id": "DjFZPHOlr_BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "const int FACTOR = 2;\n",
        "const int GRID_DIM_X = (1 << 8) / FACTOR;\n",
        "const int BLOCK_DIM_X = 1 << 8;\n",
        "const int NUM_ELEMENTS = GRID_DIM_X * BLOCK_DIM_X * FACTOR;\n",
        "const size_t NUM_BYTES = sizeof(int) * NUM_ELEMENTS;\n",
        "const size_t NUM_BYTES_PARTIAL = sizeof(int) * GRID_DIM_X;\n",
        "const size_t NUM_BYTES_SUM = sizeof(int);\n",
        "\n",
        "__device__ void reduceWrap(volatile int *shared, int ti) {\n",
        "    shared[ti] += shared[ti + 32];\n",
        "    shared[ti] += shared[ti + 16];\n",
        "    shared[ti] += shared[ti + 8];\n",
        "    shared[ti] += shared[ti + 4];\n",
        "    shared[ti] += shared[ti + 2];\n",
        "    shared[ti] += shared[ti + 1];\n",
        "}\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *vectorSum) {\n",
        "    // Step 0: Get the vector index and the thread index.\n",
        "    int ti = threadIdx.x;\n",
        "    int vi = blockIdx.x * (blockDim.x * 2) + threadIdx.x;\n",
        "\n",
        "    // Step 1: Move elements from the vector to the cache.\n",
        "    __shared__ int shared[BLOCK_DIM_X];\n",
        "    shared[ti] = vector[vi] + vector[vi + blockDim.x];\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Accumulate vector elements.\n",
        "    for (int si = BLOCK_DIM_X / 2; si > 32; si >>= 1) {\n",
        "        if (ti < si) {\n",
        "            shared[ti] += shared[ti + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (ti < 32) {\n",
        "        reduceWrap(shared, ti);\n",
        "    }\n",
        "\n",
        "    // Step 3: Move results from the cache to the vector.\n",
        "    if (ti == 0) {\n",
        "        vectorSum[blockIdx.x] = shared[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 1: Init vector memories on both the cpu and the gpu.\n",
        "    int *h_vector = (int*) malloc(NUM_BYTES);\n",
        "    int *h_vectorSum = (int*) malloc(NUM_BYTES_SUM);\n",
        "    std::fill_n(h_vector, NUM_ELEMENTS, 1);\n",
        "\n",
        "    int *d_vector, *d_vectorPartialSum, *d_vectorSum;\n",
        "    cudaMalloc(&d_vector, NUM_BYTES);\n",
        "    cudaMalloc(&d_vectorPartialSum, NUM_BYTES_PARTIAL);\n",
        "    cudaMalloc(&d_vectorSum, NUM_BYTES_SUM);\n",
        "\n",
        "    // Step 2: Launch the kernel function to accumulate values.\n",
        "    cudaMemcpy(d_vector, h_vector, NUM_BYTES, cudaMemcpyHostToDevice);\n",
        "    reduceSum<<<GRID_DIM_X, BLOCK_DIM_X>>>(d_vector, d_vectorPartialSum);\n",
        "    reduceSum<<<1, GRID_DIM_X / 2>>>(d_vectorPartialSum, d_vectorSum);\n",
        "    cudaMemcpy(h_vectorSum, d_vectorSum, NUM_BYTES_SUM, cudaMemcpyDeviceToHost);\n",
        "    printf(\"Accumulated result is %d.\\n\", *h_vectorSum);\n",
        "\n",
        "    // Step 3: Clear vector memories.\n",
        "    free(h_vector);\n",
        "    free(h_vectorSum);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_vectorPartialSum);\n",
        "    cudaFree(d_vectorSum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_5KBPFSsEpd",
        "outputId": "30fc8dc8-eb6d-4f1b-a2d3-9dacaa66e202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated result is 0.\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice"
      ],
      "metadata": {
        "id": "g53NlTOjxT8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "const int GRID_DIM_X = (1 << 8) / 4;\n",
        "const int BLOCK_DIM_X = 1 << 8;\n",
        "const int NUM_ELEMENTS = GRID_DIM_X * BLOCK_DIM_X * 4;\n",
        "const size_t NUM_BYTES = sizeof(int) * NUM_ELEMENTS;\n",
        "const size_t NUM_BYTES_PARTIAL = sizeof(int) * GRID_DIM_X;\n",
        "const size_t NUM_BYTES_SUM = sizeof(int);\n",
        "\n",
        "__device__ void reduceWarp(volatile int *cache, int ti) {\n",
        "    cache[ti] += cache[ti + 32];\n",
        "    cache[ti] += cache[ti + 16];\n",
        "    cache[ti] += cache[ti + 8];\n",
        "    cache[ti] += cache[ti + 4];\n",
        "    cache[ti] += cache[ti + 2];\n",
        "    cache[ti] += cache[ti + 1];\n",
        "}\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *vectorSum) {\n",
        "    // Step 0: Get the vector element index and the thread index.\n",
        "    int ti = threadIdx.x;\n",
        "    int vi = blockIdx.x * (blockDim.x * 4) + threadIdx.x;\n",
        "\n",
        "    // Step 1: Move elements from vector to cache.\n",
        "    __shared__ int cache[BLOCK_DIM_X];\n",
        "    cache[ti] = (\n",
        "        vector[vi] +\n",
        "        vector[vi + blockDim.x] +\n",
        "        vector[vi + 2 * blockDim.x] +\n",
        "        vector[vi + 3 * blockDim.x]\n",
        "    );\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Accumulate vector elements.\n",
        "    for (int si = blockDim.x / 2; si > 32; si >>= 1) {\n",
        "        if (ti < si) {\n",
        "            cache[ti] += cache[ti + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (ti < 32) {\n",
        "        reduceWarp(cache, ti);\n",
        "    }\n",
        "\n",
        "    // Step 3: Move results from cache to vector.\n",
        "    if (ti == 0) {\n",
        "        vectorSum[blockIdx.x] = cache[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 1: Init vector memories on both cpu and gpu.\n",
        "    int *h_vector = (int*) malloc(NUM_BYTES);\n",
        "    int *h_vectorSum = (int*) malloc(NUM_BYTES_SUM);\n",
        "    std::fill_n(h_vector, NUM_ELEMENTS, 1);\n",
        "\n",
        "    int *d_vector, *d_vectorPartial, *d_vectorSum;\n",
        "    cudaMalloc(&d_vector, NUM_BYTES);\n",
        "    cudaMalloc(&d_vectorPartial, NUM_BYTES_PARTIAL);\n",
        "    cudaMalloc(&d_vectorSum, NUM_BYTES_SUM);\n",
        "\n",
        "    // Step 2: Launch the kernel function to accumulate all elements.\n",
        "    cudaMemcpy(d_vector, h_vector, NUM_BYTES, cudaMemcpyHostToDevice);\n",
        "    reduceSum<<<GRID_DIM_X, BLOCK_DIM_X>>>(d_vector, d_vectorPartial);\n",
        "    reduceSum<<<1, GRID_DIM_X / 4>>>(d_vectorPartial, d_vectorSum);\n",
        "    cudaMemcpy(h_vectorSum, d_vectorSum, NUM_BYTES_SUM, cudaMemcpyDeviceToHost);\n",
        "    printf(\"Accumulated result is %d.\\n\", *h_vectorSum);\n",
        "\n",
        "    // Step 3: Clear vector memories.\n",
        "    free(h_vector);\n",
        "    free(h_vectorSum);\n",
        "    cudaFree(d_vector);\n",
        "    cudaFree(d_vectorPartial);\n",
        "    cudaFree(d_vectorSum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XABY1m3kxVYq",
        "outputId": "c4f22247-b0cb-48a9-b2b4-c5571b5b38c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated result is 0.\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6: CUDA Parallel Reduction Part 6\n",
        "\n",
        "## Optimizations"
      ],
      "metadata": {
        "id": "FCnqpyrAEFSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cooperative_groups.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "using namespace cooperative_groups;\n",
        "using namespace std;\n",
        "\n",
        "__device__ int threadSum(int *vector, int numElements) {\n",
        "    int sum = 0;\n",
        "    int g_ti = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int g_tc = gridDim.x * blockDim.x;\n",
        "\n",
        "    for (int i = g_ti; i < numElements / 4; i += g_tc) {\n",
        "        int4 element = ((int4*) vector)[i];\n",
        "        sum += element.x + element.y + element.z + element.w;\n",
        "    }\n",
        "\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "__device__ int blockSum(thread_group& g, int *cache, int sumOfThread) {\n",
        "    // Step 0: Get the thread index.\n",
        "    int b_ti = g.thread_rank();\n",
        "\n",
        "    // Step 1: Move values to cache.\n",
        "    cache[b_ti] = sumOfThread;\n",
        "    g.sync();\n",
        "\n",
        "    // Step 2: Accumulate values with divide-and-conquer approach.\n",
        "    for (int si = g.size() / 2; si > 0; si >>= 1) {\n",
        "        if (b_ti < si) {\n",
        "            cache[b_ti] += cache[b_ti + si];\n",
        "        }\n",
        "        g.sync();\n",
        "    }\n",
        "\n",
        "    // Step 3: Return results.\n",
        "    return cache[0];\n",
        "}\n",
        "\n",
        "__device__ void gridSum(thread_group& g, int *sum, int sumOfBlock) {\n",
        "    if (g.thread_rank() == 0) {\n",
        "        atomicAdd(sum, sumOfBlock);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *sum, int numElements) {\n",
        "    // Step 0: Set up parameters and shared caches.\n",
        "    extern __shared__ int cache[];\n",
        "    auto g = this_thread_block();\n",
        "\n",
        "    // Step 1: Sum up all elements in the vector.\n",
        "    int sumOfThread = threadSum(vector, numElements);\n",
        "    int sumOfBlock = blockSum(g, cache, sumOfThread);\n",
        "    gridSum(g, sum, sumOfBlock);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 0: Set up parameters.\n",
        "    int threadFactor = 4;\n",
        "    int gridDimX = 1 << 8;\n",
        "    int blockDimX = (1 << 8) / threadFactor;\n",
        "    int numMemElements = gridDimX * blockDimX * threadFactor;\n",
        "    size_t numMemBytes = sizeof(int) * numMemElements;\n",
        "    size_t numCacheBytes = sizeof(int) * blockDimX;\n",
        "\n",
        "    // Step 1: Init memories.\n",
        "    int *vector, *sum;\n",
        "    cudaMallocManaged(&vector, numMemBytes);\n",
        "    cudaMallocManaged(&sum, sizeof(int));\n",
        "    fill_n(vector, numMemElements, 1);\n",
        "\n",
        "    // Step 2: Accumulate vector elements.\n",
        "    reduceSum<<<gridDimX, blockDimX, numCacheBytes>>>(vector, sum, numMemElements);\n",
        "    cudaDeviceSynchronize();\n",
        "    printf(\"Accumulated result is %d.\\n\", sum[0]);\n",
        "\n",
        "    // Step 3: Clear memories.\n",
        "    cudaFree(vector);\n",
        "    cudaFree(sum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9tZyybrEMji",
        "outputId": "220aade2-ab93-4935-83a0-8968049a3773"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated result is 65536.\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice 1\n",
        "\n",
        "with `<cooperative_groups.h>`"
      ],
      "metadata": {
        "id": "HoWJVC_zDqD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <assert.h>\n",
        "#include <algorithm>\n",
        "#include <cooperative_groups.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "using namespace cooperative_groups;\n",
        "using namespace std;\n",
        "\n",
        "__device__ int threadSum(int *vector, int m_numElements) {\n",
        "    int sum = 0;\n",
        "    int g_ti = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int g_tc = gridDim.x * blockDim.x;\n",
        "\n",
        "    for (int vi = g_ti; vi < m_numElements / 4; vi += g_tc) {\n",
        "        int4 element4 = ((int4*) vector)[vi];\n",
        "        sum += element4.x + element4.y + element4.z + element4.w;\n",
        "    }\n",
        "\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "__device__ int blockSum(thread_group &group, int sumOfThread, int *cache) {\n",
        "    // Step 0: Get the thread index.\n",
        "    int b_ti = group.thread_rank();\n",
        "\n",
        "    // Step 1: Move values to the cache.\n",
        "    cache[b_ti] = sumOfThread;\n",
        "    group.sync();\n",
        "\n",
        "    // Step 2: Divide-and-conquer the thread sum in one block.\n",
        "    for (int si = group.size() / 2; si > 0; si >>= 1) {\n",
        "        if (b_ti < si) {\n",
        "            cache[b_ti] += cache[b_ti + si];\n",
        "        }\n",
        "        group.sync();\n",
        "    }\n",
        "\n",
        "    // Step 3: Return the sum of the block.\n",
        "    return cache[b_ti];\n",
        "}\n",
        "\n",
        "__device__ void gridSum(thread_group &group, int *sum, int sumOfBlock) {\n",
        "    if (group.thread_rank() == 0) {\n",
        "        atomicAdd(sum, sumOfBlock);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *sum, int m_numElements) {\n",
        "    // Step 0: Set up parameters and shared cache.\n",
        "    extern __shared__ int cache[];\n",
        "    thread_group group = this_thread_block();\n",
        "\n",
        "    // Step 1: Sum up elements in thread-block-grid levels.\n",
        "    int sumOfThread = threadSum(vector, m_numElements);\n",
        "    int sumOfBlock = blockSum(group, sumOfThread, cache);\n",
        "    gridSum(group, sum, sumOfBlock);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 0: Set up parameters.\n",
        "    int d_numThreadTask = 4;\n",
        "    int d_gridDimX = 1 << 8;\n",
        "    int d_blockDimX = (1 << 8) / d_numThreadTask;\n",
        "    int m_numElements = d_gridDimX * d_blockDimX * d_numThreadTask;\n",
        "    size_t m_numBytes = sizeof(int) * m_numElements;\n",
        "    size_t c_numBytes = sizeof(int) * d_blockDimX;\n",
        "\n",
        "    // Step 1: Init memories.\n",
        "    int *vector, *sum;\n",
        "    cudaMallocManaged(&vector, m_numBytes);\n",
        "    cudaMallocManaged(&sum, sizeof(int));\n",
        "    fill_n(vector, m_numElements, 1);\n",
        "\n",
        "    printf(\"vector[0] = %d\\n\", vector[0]);\n",
        "\n",
        "    // Step 2: Launch the kernel function to aggregate numbers.\n",
        "    reduceSum<<<d_gridDimX, d_blockDimX, c_numBytes>>>(vector, sum, m_numElements);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"Expected: *sum = 65536\\n\");\n",
        "    printf(\"Actual: *sum = %d\\n\", *sum);\n",
        "    assert(*sum == 1 << 16);\n",
        "\n",
        "    // Step 3: Clear memories.\n",
        "    cudaFree(vector);\n",
        "    cudaFree(sum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baXXoGV8DrmK",
        "outputId": "38b47276-94ec-4a1b-f5d3-a7311d3deb48"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector[0] = 1\n",
            "Expected: *sum = 65536\n",
            "Actual: *sum = 65536\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice 2\n",
        "\n",
        "without `<cooperative_groups.h>`"
      ],
      "metadata": {
        "id": "jy3cVtNqOH1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <assert.h>\n",
        "#include <algorithm>\n",
        "#include <stdio.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__device__ int threadSum(int *vector, int m_numElements) {\n",
        "    int sum = 0;\n",
        "    int g_ti = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int g_tc = gridDim.x * blockDim.x;\n",
        "\n",
        "    for (int vi = g_ti; vi < m_numElements / 4; vi += g_tc) {\n",
        "        int4 element4 = ((int4*) vector)[vi];\n",
        "        sum += element4.x + element4.y + element4.z + element4.w;\n",
        "    }\n",
        "\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "__device__ int blockSum(int sumOfThread, int *cache) {\n",
        "    // Step 0: Get the thread index.\n",
        "    int b_ti = threadIdx.x;\n",
        "\n",
        "    // Step 1: Move values to the cache.\n",
        "    cache[b_ti] = sumOfThread;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Divide-and-conquer the thread sum in one block.\n",
        "    for (int si = blockDim.x / 2; si > 0; si >>= 1) {\n",
        "        if (b_ti < si) {\n",
        "            cache[b_ti] += cache[b_ti + si];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Step 3: Return the sum of the block.\n",
        "    return cache[b_ti];\n",
        "}\n",
        "\n",
        "__device__ void gridSum(int *sum, int sumOfBlock) {\n",
        "    if (threadIdx.x == 0) {\n",
        "        atomicAdd(sum, sumOfBlock);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void reduceSum(int *vector, int *sum, int m_numElements) {\n",
        "    // Step 0: Set up parameters and shared cache.\n",
        "    extern __shared__ int cache[];\n",
        "\n",
        "    // Step 1: Sum up elements in thread-block-grid levels.\n",
        "    int sumOfThread = threadSum(vector, m_numElements);\n",
        "    int sumOfBlock = blockSum(sumOfThread, cache);\n",
        "    gridSum(sum, sumOfBlock);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Step 0: Set up parameters.\n",
        "    int d_numThreadTask = 4;\n",
        "    int d_gridDimX = 1 << 8;\n",
        "    int d_blockDimX = (1 << 8) / d_numThreadTask;\n",
        "    int m_numElements = d_gridDimX * d_blockDimX * d_numThreadTask;\n",
        "    size_t m_numBytes = sizeof(int) * m_numElements;\n",
        "    size_t c_numBytes = sizeof(int) * d_blockDimX;\n",
        "\n",
        "    // Step 1: Init memories.\n",
        "    int *vector, *sum;\n",
        "    cudaMallocManaged(&vector, m_numBytes);\n",
        "    cudaMallocManaged(&sum, sizeof(int));\n",
        "    fill_n(vector, m_numElements, 1);\n",
        "\n",
        "    printf(\"vector[0] = %d\\n\", vector[0]);\n",
        "\n",
        "    // Step 2: Launch the kernel function to aggregate numbers.\n",
        "    reduceSum<<<d_gridDimX, d_blockDimX, c_numBytes>>>(vector, sum, m_numElements);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"Expected: *sum = 65536\\n\");\n",
        "    printf(\"Actual: *sum = %d\\n\", *sum);\n",
        "    assert(*sum == 1 << 16);\n",
        "\n",
        "    // Step 3: Clear memories.\n",
        "    cudaFree(vector);\n",
        "    cudaFree(sum);\n",
        "\n",
        "    printf(\"Success!\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pn8sHbrOI9-",
        "outputId": "361dff74-f9ad-48d1-9e14-c578a46093c1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector[0] = 1\n",
            "Expected: *sum = 65536\n",
            "Actual: *sum = 65536\n",
            "Success!\n"
          ]
        }
      ]
    }
  ]
}